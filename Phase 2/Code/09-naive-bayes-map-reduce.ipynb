{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 10:56:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('NaiveBayes').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_data = spark.read.csv('dataset/05-naive-bayes/train.csv', header=True, inferSchema=True)\n",
    "training_data  = training_data.rdd\n",
    "num_features = len(training_data.first()) - 1\n",
    "training_data = training_data.map(lambda x: (x[:-1], int(x[-1])))\n",
    "\n",
    "# # # Calculate the class priors\n",
    "class_counts = training_data.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "total_count = training_data.count()\n",
    "\n",
    "class_priors = class_counts.map(lambda x: (x[0], x[1] / float(total_count)))\n",
    "\n",
    "# calculate the feature counts by class\n",
    "feature_counts_by_class = training_data.flatMap(lambda x: [(i, x[0][i], x[1]) for i in range(len(x[0]))])\n",
    "\n",
    "feature_counts_by_class = feature_counts_by_class.map(lambda x: ((x[0], x[1], x[2]), 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# sort by the column index and then the class\n",
    "sorted_features = feature_counts_by_class.sortBy(lambda x: (x[0][0], x[0][2]))\n",
    "\n",
    "class_counts_list = class_counts.collect()\n",
    "\n",
    "class_counts_dict = dict(class_counts_list)\n",
    "\n",
    "# Calculate the conditional probabilities for each feature and class\n",
    "conditional_probs = feature_counts_by_class.map(lambda x: ((x[0][0], x[0][1], x[0][2]), (x[1], class_counts_dict[x[0][2]])))\n",
    "conditional_probs = conditional_probs.sortBy(lambda x: (x[0][0], x[0][2]))\n",
    "\n",
    "conditional_probs = conditional_probs.mapValues(lambda x: (x[0] + 1) / (x[1] + num_features))\n",
    "\n",
    "# Sort by the column index and then the class\n",
    "sorted_probs = conditional_probs.sortBy(lambda x: (x[0][0], x[0][2]))\n",
    "\n",
    "# Convert the probabilities to a dictionary for faster lookup\n",
    "\n",
    "prob_dict = sorted_probs.collectAsMap()\n",
    "\n",
    "# convert the dict to have the class as the key\n",
    "prob_dict = sorted_probs.map(lambda x: (x[0][2], ((x[0][0], x[0][1]), x[1]))).groupByKey().mapValues(list).collectAsMap()\n",
    "\n",
    "# convert the dict to have the feature as the key in the inner dict and value and prob as the value\n",
    "prob_dict = sorted_probs.map(lambda x: (x[0][2], ((x[0][0], x[0][1]), x[1]))).groupByKey().mapValues(list).collectAsMap()\n",
    "\n",
    "class_priors = class_priors.collectAsMap() \n",
    "\n",
    "new_dict = {}\n",
    "\n",
    "for key, value in prob_dict.items():\n",
    "    new_dict[key] = {}\n",
    "    for sub_value in value:\n",
    "        inner_key = sub_value[0][0]\n",
    "        inner_value = (sub_value[0][1], sub_value[1])\n",
    "        if inner_key in new_dict[key]:\n",
    "            new_dict[key][inner_key].append(inner_value)\n",
    "        else:\n",
    "            new_dict[key][inner_key] = [inner_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "# split the testing data into features and labels\n",
    "def classify(data):\n",
    "    features = data[0]\n",
    "    actual_label = data[1]\n",
    "    predicted_label = None\n",
    "    max_posterior = float('-inf')\n",
    "\n",
    "    for class_label in class_priors.keys():\n",
    "        posterior = math.log(class_priors[class_label])\n",
    "        # loop over the features and index\n",
    "        for feature, feature_value in enumerate(features):\n",
    "            # get the conditional probability for the feature value and class label\n",
    "            for value, prob in new_dict[class_label][feature]:\n",
    "                if value == feature_value:\n",
    "                    posterior += math.log(prob)\n",
    "                    break\n",
    "        if posterior > max_posterior:\n",
    "            max_posterior = posterior\n",
    "            predicted_label = class_label\n",
    "\n",
    "    return (actual_label, predicted_label)\n",
    "\n",
    "testing_data = spark.read.csv('dataset/05-naive-bayes/test.csv', header=True, inferSchema=True).rdd\n",
    "testing_data = testing_data.map(lambda x: (x[:-1], int(x[-1])))\n",
    "predictions = testing_data.map(classify)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "correct_predictions = predictions.filter(lambda x: x[0] == x[1]).count()\n",
    "accuracy = correct_predictions / float(testing_data.count())\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
